# Case T√©cnico - Analytics Engineer @ Dadosfera

**Candidato:** Lucas Carvalho Soares da Silva.

**Data:** Janeiro de 2026.

**Dataset:** Brazilian E-Commerce Public Dataset (Olist).

**link:** https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce

# üìÇ Estrutura do Reposit√≥rio

    ‚Ä¢ /notebooks: Cont√©m o c√≥digo de tratamento e GenAI.
    
    ‚Ä¢ /img: prints.
    
    ‚Ä¢ README.md: Documenta√ß√£o principal.

# üõ†Ô∏è Stack Utilizada:

    Data Platform: Dadosfera (Coleta, Explorar, Analisar)

    Linguagens: Python(Processamento e Tratamento via Pandas, e Quality via Pandera), SQL(Modelagem Star Schema)

    IA: LLM para enriquecimento de dados(GenAI via API do Gemini)

    Visualiza√ß√£o: Metabase & Streamlit

# üìñ Dicion√°rio de Dados (Principais Entidades e Atributos de Valor)

Foco na documenta√ß√£o das colunas que trazem intelig√™ncia anal√≠tica para o case:

| Coluna | Tipo | Descri√ß√£o |
| :--- | :--- | :--- |
| `order_id` | PK (String) | Identificador √∫nico do pedido. |
| `order_purchase_timestamp` | Datetime | Data e hora em que a compra foi realizada. |
| `order_hour` | String | Feature Engineering: Hor√°rio formatado (HH:mm) para an√°lise de pico. |
| `delivery_diff_days` | Integer | Feature Engineering: Dias de diferen√ßa entre entrega real e estimada. |
| `lead_time_days` | Integer | Feature Engineering: Quantidade de dias para a realiza√ß√£o da entrega. |
| `genai_category` | String | Intelig√™ncia Artificial: Categoria refinada via LLM (Gemini/OpenAI). |

# üìë Sum√°rio Executivo

Este projeto visa a implementa√ß√£o de uma plataforma de dados ponta a ponta utilizando a Dadosfera.

O foco √© transformar dados brutos de e-commerce em ativos de intelig√™ncia de neg√≥cio, utilizando modelagem dimensional e enriquecimento via Intelig√™ncia Artificial(GenAI), entregando an√°lises descritivas e prescritivas com agilidade e menor custo em todas as √°reas da empresa.

# ‚ö†Ô∏è Nota sobre a Metodologia de Execu√ß√£o

**Observa√ß√£o T√©cnica:** Para garantir a m√°xima integridade e efici√™ncia no carregamento de dados, optei por realizar os itens 4 (Data Quality) e 5 (Enriquecimento GenAI) previamente √† etapa de 2 (Integra√ß√£o).

**Motiva√ß√£o:** Tratar os dados em Python e enriquec√™-los com LLM antes da ingest√£o permite que o Data Lakehouse receba arquivos otimizados em .parquet, reduzindo custos de armazenamento, evitando processamento de dados nulos e garantindo que o Cat√°logo de Dados (Item 3) j√° nas√ßa com as features de intelig√™ncia artificial integradas.

**Narrativa de Neg√≥cio:** O projeto simula a fase p√≥s-kickoff de uma implementa√ß√£o real para uma grande empresa de e-commerce.

# üìã Itens do Case

## **0. Planejamento e Metodologia √Ågil**

Organiza√ß√£o do projeto utilizando Kanban para gest√£o de tarefas e prazos.

**Link:** https://trello.com/invite/b/69764932449c1987ea2c18b9/ATTI3248d59ac8be331a71a0f6fa3e88fcf36E8F88A2/planejamento-case-tecnico-dadosfera

![Planejamento √Ågil](img/planejamento_trello.png)
*Legenda: Board Kanban estruturado para o ciclo de vida do projeto de Analytics Engineering.*

## **1. Sele√ß√£o do Dataset**

Escolha de uma base real de e-commerce com mais de 100k registros para garantir a escalabilidade da solu√ß√£o.

**link:** https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce

![Dataset](img/dataset_kaggle.png)
*Legenda: Dataset Olist selecionado pela sua complexidade relacional e volume de dados (+100k pedidos).*

## **2. Integra√ß√£o (M√≥dulo Integrar)**

Os dados foram ingeridos na plataforma Dadosfera utilizando o m√≥dulo Integrar, onde foram criados pipelines de upload para arquivos Parquet, garantindo a integridade dos tipos de dados e a documenta√ß√£o inicial dos metadados.

**Link:** https://app.dadosfera.ai/pt-BR/collect/import-files

![Integra√ß√£o](img/importacao_tabelas_sucesso.png)
*Legenda: Registro do pipeline de ingest√£o no m√≥dulo Integrar. A imagem confirma o sucesso no upload dos arquivos em formato .parquet, garantindo a preserva√ß√£o dos schemas e a otimiza√ß√£o do armazenamento no Data Lakehouse.*

## **3. Cataloga√ß√£o(M√≥dulo Explorar)**

Ap√≥s a integra√ß√£o, os dados foram registrados como ativos oficiais no m√≥dulo Explorar da Dadosfera. Esta etapa foi fundamental para garantir a transpar√™ncia da linhagem dos dados e a documenta√ß√£o das regras de neg√≥cio aplicadas.

Durante a etapa de cataloga√ß√£o no m√≥dulo Explorar, identifiquei que a coluna original PRODUCT_CATEGORY_NAME apresentava 610 valores nulos (conforme evidenciado nos indicadores de qualidade da plataforma). Para garantir a integridade anal√≠tica, documentei a estrutura das tabelas conforme abaixo:

### **Tabela: tb_olist_products_enriched (Dimens√£o de Produtos)**

Esta tabela representa o maior ganho de governan√ßa do projeto, onde a Intelig√™ncia Artificial foi utilizada para tratar falhas de preenchimento da base original.

**Link:** https://app.dadosfera.ai/pt-BR/catalog/data-assets/6a5f51c8-7099-459f-83c8-f8a3746af8f6

    Destaque de Governan√ßa: A coluna original PRODUCT_CATEGORY_NAME apresentava 610 valores nulos.

    Solu√ß√£o: Foi criada a coluna GENAI_CATEGORY via LLM (Gemini 1.5 Flash). Esta coluna possui 0% de nulos, garantindo que 100% dos produtos agora possuem uma categoria sem√¢ntica v√°lida para an√°lise no Dashboard.
    
| Coluna | Descri√ß√£o | Nota de Governan√ßa |
| :--- | :--- | :--- |
| `PRODUCT_ID` | Chave prim√°ria do produto | Identificador √∫nico. |
| `PRODUCT_CATEGORY_NAME` |	Categoria original | Cont√©m inconsist√™ncias e nulos. |
| `GENAI_CATEGORY` | Categoria via IA |	Feature criada via LLM para normalizar a base e tratar os 610 nulos. |

Nota: A documenta√ß√£o foi espelhada neste README para garantir a linhagem dos dados fora da camada de processamento.

![Cataloga√ß√£o](img/tabela_products_genai.png)
*Legenda: An√°lise de integridade e completude de dados. O painel de Data Quality evidencia a efic√°cia da estrat√©gia de IA: enquanto a categoria original apresenta lacunas (610 nulos), a coluna enriquecida via GenAI entrega 100% de preenchimento, eliminando o ru√≠do anal√≠tico.*

### **Tabela: tb_olist_orders_processed (Tabela Fato)**

Centraliza as m√©tricas de performance log√≠stica calculadas durante a fase de engenharia.

**Link:** https://app.dadosfera.ai/pt-BR/catalog/data-assets/c3fcdfa8-8129-4285-9d85-4fb8a4efd850

Colunas Enriquecidas:

    LEAD_TIME: Diferen√ßa em dias entre a compra e a entrega real.

    DELIVERY_PERFORMANCE: Diferen√ßa entre a data prevista e a entrega real (atraso/antecipa√ß√£o).

    SEASONALITY_FLAG: Classifica√ß√£o temporal dos pedidos (ex: Black Friday, Natal).

![Tabela](img/tb_orders.png)

### **Tabela: tb_olist_customers (Dimens√£o de Clientes)**

**Link:** https://app.dadosfera.ai/pt-BR/catalog/data-assets/1e1df43f-48d5-4723-938c-8da87b82f7d0

    Uso: Fornece a granularidade geogr√°fica necess√°ria para o mapeamento de calor das vendas por estado e cidade.

    Status de Qualidade: 100% de completude nos campos de localiza√ß√£o.

![Tabela](img/tb_customers.png)

## **4. Processamento de Dados & Data Quality**

Aplica√ß√£o de limpeza, tratamento de tipos e testes de qualidade via Python(Notebook anexo).

**Link:** https://colab.research.google.com/github/Luckaz7/LUCAS_CARVALHO_DDF_TECH_012026/blob/main/notebooks/processamento_dados.ipynb

![Processamento e Data Quality](img/teste_qualidade_dados_brutos_pedidos.png)
![Processamento e Data Quality](img/teste_qualidade_dados_brutos_clientes.png)
![Processamento e Data Quality](img/teste_qualidade_dados_brutos_produtos.png)
*Legenda: Auditoria de dados via Python(Pandera) identificando integridade de chaves prim√°rias e tratamento de valores nulos.*

Verifica√ß√£o de Data Quality ap√≥s corre√ß√£o:

![Processamento e Data Quality](img/teste_qualidade_dados_silver_pedidos.png)
*Legenda: Nova auditoria de dados via Python(Pandera) corrigindo as falhas de integridade encontrada nos dados.*

## **5. Intelig√™ncia de Dados(GenAI)**

Enriquecimento da base original utilizando modelos de linguagem para categoriza√ß√£o inteligente.

**Link:** https://colab.research.google.com/github/Luckaz7/LUCAS_CARVALHO_DDF_TECH_012026/blob/main/notebooks/processamento_dados.ipynb

**Nota de Implementa√ß√£o(GenAI via API do Gemini)**: Durante o enriquecimento, identifiquei respostas nulas da API devido aos filtros de seguran√ßa padr√£o(Safety Settings), onde implementei um tratamento de exce√ß√µes no pipeline Python para garantir a continuidade da ingest√£o, mapeando retornos inv√°lidos temporariamente como 'N√£o Mapeado' para manter a integridade do schema no carregamento para a Dadosfera.

![Intelig√™ncia de Dados(GenAI)](img/enriquecimento_dados_genai.png)
*Legenda: Extra√ß√£o de atributos de produtos via LLM para maior granularidade na an√°lise de vendas.*

## **6. Modelagem**

Para a estrutura√ß√£o dos dados, utilizei o Editor de Consultas Visual (No-Code) da plataforma, sendo fundamental para conectar a tabela fato de pedidos com as dimens√µes de clientes e produtos enriquecidos, criando uma camada sem√¢ntica pronta para an√°lise.

A√ß√µes Realizadas:

    Cria√ß√£o de jun√ß√µes (Left Joins) entre a tabela TB_OLIST_ORDERS_PROCESSED e TB_OLIST_CUSTOMERS utilizando a chave Customer ID.

    Estabelecimento de rela√ß√µes para permitir o cruzamento de m√©tricas geogr√°ficas com o status operacional dos pedidos.

Performance: Devido √† volumetria de dados (Big Data), as consultas foram otimizadas atrav√©s de agrega√ß√µes diretas para garantir um tempo de resposta eficiente no Dashboard.

![Modelagem](img/modelagem_relacional.png) 
*Legenda: Configura√ß√£o visual do Join entre a tabela fato de pedidos e a dimens√£o de clientes.*

## **7. Visualiza√ß√£o**

Nota: Para a visualiza√ß√£o dos dados, utilizei o Metabase integrado. Devido √† alta volumetria do dataset Olist, optei por criar visualiza√ß√µes segmentadas por ativos de dados para garantir a melhor performance de resposta e estabilidade do dashboard(SLA de visualiza√ß√£o).

**Link:** https://metabase-treinamentos.dadosfera.ai/collection/1029-lucas-carvalho-case-tecnico-analytics-engineer

![Vizualiza√ß√£o](img/cole√ß√£o_analises.png)
*Legenda: visualiza√ß√µes segmentadas por ativos de dados.*

O resultado final foi consolidado em um Dashboard Executivo com 5 visualiza√ß√µes din√¢micas, oferecendo uma vis√£o 360¬∫ da opera√ß√£o.

### **7.1 Constru√ß√£o do Star Schema(Kimball)**

A modelagem foi estruturada seguindo a metodologia Star Schema de Ralph Kimball, onde defini a tabela de pedidos como a Fato, conectando-a √†s Dimens√µes de Clientes e Produtos, uma vez que essa arquitetura permite que as m√©tricas de neg√≥cio(como volume de vendas e lead time) sejam filtradas por qualquer atributo das dimens√µes, como localiza√ß√£o geogr√°fica ou a nova categoria gerada por GenAI.

    Tabela Fato: TB_OLIST_ORDERS_PROCESSED como o centro da an√°lise(o evento de neg√≥cio: o pedido);

    Tabelas Dimens√£o: Conec√ß√£o da Tabela Fato √†s dimens√µes CUSTOMERS e PRODUCTS_ENRICHED(enriquecida por GenAI);

    Relacionamentos: Foi estabelecida as chaves(Customer ID e Product ID) para criar o relacionamento 1:N, sendo uma defini√ß√£o cl√°ssica de Star Schema.

### **7.2 KPIs e Visualiza√ß√µes Criadas:**

**Link Dashboard:** https://app.dadosfera.ai/pt-BR/catalog/data-assets/b13d7eac-521a-4a83-a6b2-9c9a094d28c2

    Distribui√ß√£o de Categorias via IA(Gr√°fico de Rosca): Demonstra o sucesso do enriquecimento de dados com Gemini, categorizando 32.951 produtos.

    Distribui√ß√£o de Status de Pedidos(Gr√°fico de Barras): Vis√£o operacional da sa√∫de das entregas, com destaque para 96.478 pedidos entregues.

    Tend√™ncia de Vendas Mensal(Gr√°fico de Linha): Identifica√ß√£o de picos de demanda ao longo do tempo.

    Top 10 Estados com mais Clientes(Gr√°fico de Barras): Intelig√™ncia geogr√°fica revelando a domin√¢ncia do estado de SP no volume de clientes.

    Lead Time M√©dio por Status(Gr√°fico de Rosca): M√©trica de efici√™ncia log√≠stica processada no pipeline de dados.

![Vizualiza√ß√£o](img/dashboard_final.png) 
*Legenda: Dashboard Final em Dark Mode apresentando os insights de neg√≥cio e engenharia de dados.*

### **8. Pipelines de Dados e Orquestra√ß√£o**

Devido a restri√ß√µes de conectividade entre o ambiente Sandbox da Dadosfera e os provedores Cloud(Google/Render) via protocolos de rede, a etapa de Pipeline foi documentada atrav√©s de sua Arquitetura L√≥gica, garantindo a entrega do projeto conforme os requisitos:

    Modelagem de Ingest√£o: Projetei o pipeline para conex√£o via PostgreSQL (Supabase), estruturando a extra√ß√£o dos dados transacionais da Olist.

    Pipeline de Transforma√ß√£o (ETL): O fluxo foi desenhado para realizar o saneamento em Python e o c√°lculo de m√©tricas de log√≠stica (LEAD_TIME).

    Orquestra√ß√£o de IA: Integra√ß√£o de um step de micro-transforma√ß√£o para consumo do modelo Gemini, automatizando a categoriza√ß√£o de produtos.

![Pipeline](img/bloqueio_acesso_google.png) 
*Legenda: Acesso bloqueado para conex√£o com a fonte de dados no Google Cloud Storage*

![Pipeline](img/erro_conexao_db.png) 
*Legenda: Erro de conex√£o com DB PostgreSQL via Render*

### **9. Data App(Streamlit)**

Solu√ß√£o: Como o m√≥dulo nativo da plataforma n√£o estava liberado para este usu√°rio, desenvolvi um Data App externo utilizando Streamlit e WSL2.

![Data App](img/modulo_inteligencia.png) 
*Legenda: M√≥dulo intelig√™ncia inacessivel*

Funcionalidade: O app permite filtrar produtos por categorias geradas pela IA ("Sa√∫de", etc.) e exibe insights autom√°ticos sobre o volume de produtos reclassificados, provando o valor do enriquecimento de dados. 

![Data App](img/data_app_streamlit.png) 
*Legenda: Insights autom√°ticos sobre o volume de produtos reclassificados*

### **10. Apresenta√ß√£o do Case**

    Status: Planejado.

    Link do v√≠deo com a proposta de valor e substitui√ß√£o da arquitetura legada pela Dadosfera (em breve).

    ‚Ä¢ [INSIRA O PRINT]
